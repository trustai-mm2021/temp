# Overview
Machine learning (ML) models are increasingly being employed to make highly consequential decisions pertaining to employment, bail, parole, and lending. While such models can learn from large amounts of data and are often very scalable, their applicability is limited by certain safety challenges. A key challenge is identifying and correcting systematic patterns of mistakes made by ML models before deploying them in the real world.

The goal of this workshop, held at the [2019 International Conference on Learning Representations (ICLR)](https://iclr.cc/), is to bring together researchers and practitioners with different perspectives on debugging ML models. 

## Speakers
<div style="display: flex">
  <div style="width:22.5%">
    <a href="https://people.csail.mit.edu/madry/">
    <img alt="Aleksander Madry" src="pics/aleksander.jpg">
    </a><br>
    <a href="https://people.csail.mit.edu/madry/">Aleksander Madry</a><br>
    MIT
  </div>
  
  <div style="width:2.5%">
  </div>
  
  <div style="width:22.5%">
    <a href="https://users.cs.duke.edu/~cynthia/">
    <img alt="Cynthia Rudin" src="pics/cynthia.jpg">
    </a><br>
  <a href="https://users.cs.duke.edu/~cynthia/">Cynthia Rudin</a><br>
    Duke University 
  </div>
  
  <div style="width:2.5%">
  </div>
  
  <div style="width:22.5%">
    <a href="https://ai.google/research/people/DanMoldovan">
    <img alt="Dan Moldovan" src="pics/dan.jpg">
    </a><br>
  <a href="https://ai.google/research/people/DanMoldovan">Dan Moldovan</a><br>
    Google
  </div>
  
  <div style="width:2.5%">
  </div>
  
  <div style="width:22.5%">
    <a href="https://www.linkedin.com/in/deborah-raji-065751b2/">
    <img alt="Deborah Raji" src="pics/deb.jpg">
    </a><br>
  <a href="https://www.linkedin.com/in/deborah-raji-065751b2/">Deborah Raji</a><br>
    University of Toronto
  </div>
</div>

<div style="display: flex">
  <div style="width:22.5%">
    <a href="https://obastani.github.io/">
    <img alt="Osbert Bastani" src="pics/osbert.jpg">
    </a><br>
    <a href="https://obastani.github.io/">Osbert Bastani</a><br>
    University of Pennsylvania
  </div>
  
  <div style="width:2.5%">
  </div>
  
  <div style="width:22.5%">
    <a href="http://sameersingh.org/">
    <img alt="Sameer Singh" src="pics/sameer.jpg">
    </a><br>
  <a href="http://sameersingh.org/">Sameer Singh</a><br>
    UC Irvine
  </div>

  <div style="width:2.5%">
  </div>
  
  <div style="width:22.5%">
    <a href="https://suchisaria.jhu.edu/">
    <img alt="Suchi Saria" src="pics/suchi.jpg">
    </a><br>
  <a href="https://suchisaria.jhu.edu/">Suchi Saria</a><br>
    Johns Hopkins University
  </div>

</div>

## Schedule 
See [here](https://docs.google.com/document/d/1RoeyDLNup6Ym9ZEJhnMu3z720-5U8LK3FoUfVkUoC0c) for a printable version.

| Time | Event |
| ----- | -------|
| 9.50 | Opening remarks [[video]](https://slideslive.com/38915697/opening-remarks-debugging-machine-learning-models)|
| | Session Chair: [Julius Adebayo](http://juliusadebayo.com/) (MIT) |
| 10.00 | **Invited talk** – [Aleksander Madry](https://people.csail.mit.edu/madry/) (MIT): *A New Perspective on Adversarial Perturbations* [[video]](https://slideslive.com/38915699/a-new-perspective-on-adversarial-perturbations)|
| 10:30 | **Contributed talk (Best Research Paper Award)** – [Simon Kornblith](http://simonster.com/) (Google): *Similarity of Neural Network Representations Revisited* [[video]](https://slideslive.com/38915700/similarity-of-neural-network-representations-revisited)|
| 10.40 | **Contributed talk (Best Demo Award)** – [Besmira Nushi](https://www.microsoft.com/en-us/research/people/benushi/) (Microsoft Research): *Error terrain analysis for machine learning: Tool and visualizations* [[video]](https://slideslive.com/38915701/error-terrain-analysis-for-machine-learning-tool-and-visualizations)|
| 10.50 | Coffee break |
| | Session Chair: [Julius Adebayo](http://juliusadebayo.com/) (MIT) |
| 11.10 | **Invited talk** – [Osbert Bastani](https://obastani.github.io/) (University of Pennsylvania): *Verifiable Reinforcement Learning via Policy Extraction* [[video]](https://slideslive.com/38915702/verifiable-reinforcement-learning-via-policy-extraction)|
| 11:40 | **Contributed talk (Best Student Research Paper Award)** – [Daniel Kang](https://ddkang.github.io/) (Stanford): *Debugging Machine Learning via Model Assertions* [[video]](https://slideslive.com/38915703/model-assertions-fo-quality-assirance-in-machine-learning) |
| 11:50 | **Contributed talk** – [Benjamin Link](https://www.linkedin.com/in/ben-link-45a5981a) (Indeed): *Improving Jobseeker-Employer Match Models at Indeed Through Process, Visualization, and Exploration* [[video]](https://slideslive.com/38915705/improving-jobseekeremployer-match-models-at-indeed-through-process-visualization-and-exploration)|
| 12.00 | Break|
| | Session Chair: [Sarah Tan](https://shftan.github.io/) (Cornell University / UCSF)|
| 12.10 | **Invited talk** – [Sameer Singh](http://sameersingh.org/) (UC Irvine): *Discovering Natural Bugs Using Adversarial Data Perturbations* [[video]](https://slideslive.com/38915704/discovering-natural-bugs-using-adversarial-data-perturbations)|
| 12.40 | **Invited talk** – [Deborah Raji](https://www.linkedin.com/in/deborah-raji-065751b2/) (University of Toronto): *"Debugging" Discriminatory ML Systems* [[video]](https://slideslive.com/38915706/debugging-discriminatory-ml-systems)|
| 1.00 | **Contributed talk (Best Applied Paper Award)** – [Tomer Arnon](https://www.linkedin.com/in/tomerarnon/) and [Christopher Lazarus](https://www.linkedin.com/in/christopherlazarus/): *NeuralVerification.jl: Algorithms for Verifying Deep Neural Networks* [[video]](https://slideslive.com/38915707/neuralverificationjl-algorithms-for-verifying-deep-neural-networks)|
| 1.10 | Lunch |
| 2.30 | Break |
| | Session Chair: [D Sculley](https://www.eecs.tufts.edu/~dsculley/) (Google) |
| 3.20 | Welcome back remarks |
| 3.30 | **Invited talk** – [Suchi Saria](https://suchisaria.jhu.edu/) (Johns Hopkins University): *Safe and Reliable Machine Learning: Preventing and Identifying Failures* [[video]](https://slideslive.com/38915708/safe-and-reliable-machine-learning-preventing-and-identifying-failures)|
| 4.00 | **Invited talk** – [Dan Moldovan](https://ai.google/research/people/DanMoldovan) (Google): *Better Code for Less Debugging with AutoGraph* [[video]](https://slideslive.com/38915709/better-code-for-less-debugging-with-autograph)|
| 4.20 | **Posters & Demos** & Coffee break <br> [Accepted posters](#posters) &nbsp; [Accepted demos](#demos) |
| | Session Chair: [Himabindu Lakkaraju](https://web.stanford.edu/~himalv/) (Harvard University) |
| 5.20 | **Contributed position paper** – [Michela Paganini](https://mickypaganini.github.io/) (Facebook): *The Scientific Method in the Science of Machine Learning* [[video]](https://slideslive.com/38915710/the-scientific-method-in-the-science-of-machine-learning)|
| 5.30 | **Invited opinion piece** – [Cynthia Rudin](https://users.cs.duke.edu/~cynthia/) (Duke University): *Don't debug your black box, replace it* [[video]](https://slideslive.com/38915712/dont-debug-your-black-box-replace-it)|
| 6.00 | **Q&A and panel with all invited speakers** – "The Future of ML Debugging" [[video]](https://slideslive.com/38915711/the-future-of-ml-debugging-qa-and-panel-with-all-invited-speakers) <br> Moderator: [Himabindu Lakkaraju](https://web.stanford.edu/~himalv/) (Harvard University) <br> Panelists: [Aleksander Madry](https://people.csail.mit.edu/madry/), [Cynthia Rudin](https://users.cs.duke.edu/~cynthia/), [Dan Moldovan](https://ai.google/research/people/DanMoldovan), [Deborah Raji](https://www.linkedin.com/in/deborah-raji-065751b2/), [Osbert Bastani](https://obastani.github.io/), [Sameer Singh](http://sameersingh.org/), [Suchi Saria](https://suchisaria.jhu.edu/) |
| 6.25 | Closing remarks |

## Posters
[Call for submissions](https://drive.google.com/open?id=17ccUz0F1kD9JEQC1LIrBeJNpH3xTf2w-esZWyZMjsto) (deadline has passed)
- [Discovery of Intersectional Bias in Machine Learning Using Automatic Subgroup Generation](cameraready/DebugML-19_paper_3.pdf). Angel Cabrera, Minsuk Kahng, Fred Hohman, Jamie Morgenstern and Duen Horng Chau
- [Calibration of Encoder Decoder Models for Neural Machine Translation](cameraready/DebugML-19_paper_12.pdf). Aviral Kumar and Sunita Sarawagi.
- [Step-wise Sensitivity Analysis: Identifying Partially Distributed Representations for Interpretable Deep Learning](cameraready/DebugML-19_paper_7.pdf). Botty Dimanov and Mateja Jamnik
- [Handling Bias in AI Using Simulation](cameraready/DebugML-19_paper_13.pdf). Daniel McDuff, Roger Cheng and Ashish Kapoor
- [Inverting Layers of a Large Generator](cameraready/DebugML-19_paper_18.pdf). David Bau, Jun-Yan Zhu, William Peebles, Hendrik Strobelt, Jonas Wulff, Bolei Zhou and Antonio Torralba
- [MAST: A Tool for Visualizing CNN Model Architecture Searches](cameraready/DebugML-19_paper_24.pdf). Dylan Cashman, Adam Perer and Hendrik Strobelt.
- [Visualizations of Decision Regions in the Presence of Adversarial Examples](cameraready/DebugML-19_paper_6.pdf). Grzegorz Swirszcz, Brendan O'Donoghue and Pushmeet Kohli.
- [BertViz: A Tool for Visualizing Multi-Head Self-Attention in the BERT Model](cameraready/DebugML-19_paper_2.pdf). Jesse Vig.
- [Where To Be Adversarial Perturbations Added? Investigating and Manipulating Pixel Robustness Using Input Gradients](cameraready/DebugML-19_paper_4.pdf). Jisung Hwang, Younghoon Kim, Sanghyuk Chun, Jaejun Yoo, Ji-Hoon Kim and Dongyoon Han.
- [Dissecting Pruned Neural Networks](cameraready/DebugML-19_paper_15.pdf). Jonathan Frankle and David Bau.
- [Monitoring Opaque Learning Systems](cameraready/DebugML-19_paper_25.pdf). Leilani Gilpin.
- [Model Agnostic Globally Interpretable Explanations](cameraready/DebugML-19_paper_5.pdf). Piyush Gupta, Nikaash Puri, Sukriti Verma, Pratiksha Agarwal and Balaji Krishnamurthy.
- [Debugging Trained Machine Learning Models Using Flip Points](cameraready/DebugML-19_paper_11.pdf). Roozbeh Yousefzadeh and Dianne O'Leary.
- [Universal Multi-Party Poisoning Attacks](cameraready/DebugML-19_paper_16.pdf). Saeed Mahloujifar, Ameer Mohammed and Mohammad Mahmoody.
- [Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift](cameraready/DebugML-19_paper_20.pdf). Stephan Rabanser, Stephan Guennemann and Zachary Lipton.
- [Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness](cameraready/DebugML-19_paper_19.pdf). Xiao Zhang, Saeed Mahloujifar, Mohammad Mahmoody and David Evans.
- [Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded](cameraready/DebugML-19_paper_23.pdf). Ramprasaath R. Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Dhruv Batra and Devi Parikh.
- [Similarity of Neural Network Representations Revisited](cameraready/DebugML-19_paper_9.pdf). Simon Kornblith, Mohammad Norouzi, Honglak Lee and Geoffrey Hinton (Contributed talk).
- [NeuralVerification.jl: Algorithms for Verifying Deep Neural Networks](cameraready/DebugML-19_paper_22.pdf). Changliu Liu, Tomer Arnon, Christopher Lazarus and Mykel Kochenderfer (Contributed talk).
- [Debugging Machine Learning via Model Assertions](cameraready/DebugML-19_paper_27.pdf). Daniel Kang, Deepti Raghavan, Peter Bailis and Matei Zaharia (Contributed talk).
- [The Scientific Method in the Science of Machine Learning](cameraready/DebugML-19_paper_26.pdf). Jessica Zosa Forde and Michela Paganini (Contributed talk).
